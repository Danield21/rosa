runs: 1
debug: True
train:
  epochs: 5
  lr: 5e-5
  batch_size: 10
  fraction: 1.0 # ratio of the training set to use
  seq_len: 512
  ignore_list: True # use predefined `ignore_list` consisting pf 'mlp'
  optimizer:
    name: adamw # [adam, adamw, sgd]
    params:
      momentum: 0.9
      weight_decay: 0.01
  scheduler:
    name: linear # [linear, cosine, constant, none]
    params:
      num_warmup_steps: 500
model:
  name: gpt2  # gpt2, gpt2-medium, gpt2-large, gpt2-xl
fnmodel:
  name: none # [rosa, lora, none]
  factorize_freq: 1  # factorize every n epochs
  params:
    rank: 0.01  # rank of trainable parameters [float in (0,1) or int]
    level: epoch  # ["epoch", "batch"]
    factorize_mode: random  # ["random", "top", "bottom"]
    ia3_mode: in  # ["in", "out"]
    use_scale: False  # scale factorization by rank
logging:
  print_freq: 100  # print every n batches
  eval_freq: 100  # log every n batches
  eval_level: epoch  # ["epoch", "batch"]
  input_size: [8, 512]  # used to compute latency [batch_size, seq_len]
  memory_info: True  # record memory usage
output:
  path: /home/mila/m/marawan.gamal/scratch/rosa/runs
dataset:
  name: e2e_nlg # options: eli5, e2e_nlg
  cache: /home/mila/m/marawan.gamal/.cache/huggingface
eval:
  batch_size: 8
  experiment: runs/e2e_nlg/e64_l1e-05_b32_f1.0_nsgd_m0.9_w0.01_nanone_nu100_namdistillgpt2_namelora_r0.1_leepoch_srandom_t0 # experiment name

