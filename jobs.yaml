config:
   user: marawan.gamal # your username to check slurm status
   max_jobs: 10 # maximum number of jobs to run in parallel
   max_gpus: 10 # maximum number of GPUs to use in parallel

common_preamble_declarations:
  - "#!/bin/bash"
  - "#SBATCH --output=/home/mila/m/marawan.gamal/projects/tensor-net/outputs/slurm-%j.out"
  - "#SBATCH --error=/home/mila/m/marawan.gamal/projects/tensor-net/outputs/slurm-error-%j.out"
  - "#SBATCH --mem=10G                                         # Ask for 10 GB of RAM"
  - "#SBATCH --time=8:00:00                                    # The job will run for 8 hours"
  - "#SBATCH -x cn-g[005-012,017-026]"

common_preamble_runs:
  # 1. Load the required modules
  - module load python/3.8

  # 2. Load your environment
  - source /home/mila/m/marawan.gamal/.venv/tn/bin/activate

  # 3. Copy your dataset on the compute node
  - cp -r /home/mila/m/marawan.gamal/.cache/huggingface $SLURM_TMPDIR/huggingface

groups:
  - name: MET | E2E_NLG | LoRA vs. ROSA GPT2-S
    preamble:
      - "#SBATCH --gres=gpu:2"
      - export CUDA_VISIBLE_DEVICES=0,1
    paralleljobs:
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=none
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=lora fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=lora fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=lora fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=lora fnmodel.params.rank=0.1000
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=lora fnmodel.params.rank=0.2000
      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=rosa fnmodel.params.rank=0.0125
      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=rosa fnmodel.params.rank=0.0250
      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=rosa fnmodel.params.rank=0.0500
      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=rosa fnmodel.params.rank=0.1000
      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.name=rosa fnmodel.params.rank=0.2000
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.params.sample_method=bottom fnmodel.name=rosa fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.params.sample_method=bottom fnmodel.name=rosa fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.params.sample_method=bottom fnmodel.name=rosa fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface fnmodel.params.sample_method=bottom fnmodel.name=rosa fnmodel.params.rank=0.1000

#  - name: MET | E2E_NLG | LoRA vs. ROSA GPT2-M
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - export CUDA_VISIBLE_DEVICES=0,1
#    paralleljobs:
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=none
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=0.1000
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=0.1000

#  - name: MET | E2E_NLG | LoRA vs. ROSA GPT2-L
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - export CUDA_VISIBLE_DEVICES=0,1
#    paralleljobs:
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=none
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=lora fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=lora fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=lora fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=lora fnmodel.params.rank=0.1000
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=rosa fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=rosa fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=rosa fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface train.epochs=5 model.name=gpt2-large fnmodel.name=rosa fnmodel.params.rank=0.1000

#  - name: MET | ELI5 | LoRA vs. ROSA GPT2-S
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - export CUDA_VISIBLE_DEVICES=0,1,2,3
#    paralleljobs:
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=none
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=lora fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=lora fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=lora fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=lora fnmodel.params.rank=0.1000
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=rosa fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=rosa fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=rosa fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 train.epochs=5 train.batch_size=128 train.lr=1e-3 fnmodel.name=rosa fnmodel.params.rank=0.1000

#  - name: MET | ELI5 | LoRA vs. ROSA GPT2-M
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - export CUDA_VISIBLE_DEVICES=0,1,2,3
#    paralleljobs:
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=none
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.1000
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-medium train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.1000

#  - name: MET | ELI5 | LoRA vs. ROSA GPT2-L
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - export CUDA_VISIBLE_DEVICES=0,1,2,3
#    paralleljobs:
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=none
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=lora fnmodel.params.rank=0.1000
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.0125
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.0250
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.0500
#      - python train.py dataset.cache=$SLURM_TMPDIR/huggingface dataset.name=eli5 model.name=gpt2-large train.batch_size=128 train.lr=1e-3 train.epochs=5 fnmodel.name=rosa fnmodel.params.rank=0.1000